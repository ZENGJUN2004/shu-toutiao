import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import os

# ç›®æ ‡ï¼šä¸Šæµ·å¤§å­¦æ–°é—»ç½‘
def get_header():
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'
    ]
    return {'User-Agent': random.choice(user_agents)}

def get_shu_official():
    url = "https://news.shu.edu.cn/index/zhxw.htm"
    print(f"æ­£åœ¨è®¿é—®å®˜ç½‘: {url}")
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        for link in soup.find_all('a'):
            title = link.get_text(strip=True)
            href = link.get('href')
            if title and href and len(title) > 10 and '.htm' in href:
                if not href.startswith('http'):
                    href = f"https://news.shu.edu.cn/{href.replace('../', '').lstrip('/')}"
                
                if not any(n['url'] == href for n in news_list):
                    news_list.append({"title": title, "url": href, "source": "ä¸Šå¤§å®˜ç½‘", "time": "æ ¡å†…", "tag": "official"})
    except Exception as e:
        print(f"å®˜ç½‘æŠ“å–æŠ¥é”™: {e}")
    return news_list[:8]

def get_internet_buzz():
    url = "https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd=ä¸Šæµ·å¤§å­¦"
    print(f"æ­£åœ¨å…¨ç½‘å·¡æŸ¥: {url}")
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        results = soup.find_all('div', class_='result-op')
        for item in results:
            title_tag = item.find('h3')
            if not title_tag: continue
            link_tag = title_tag.find('a')
            if not link_tag: continue
            title = link_tag.get_text(strip=True)
            href = link_tag['href']
            source_tag = item.find('span', class_='c-color-gray')
            source = source_tag.get_text(strip=True) if source_tag else "äº’è”ç½‘"
            time_tag = item.find('span', class_='c-color-gray2')
            pub_time = time_tag.get_text(strip=True) if time_tag else "è¿‘æœŸ"

            news_list.append({"title": title, "url": href, "source": source, "time": pub_time, "tag": "media"})
    except Exception as e:
        print(f"å…¨ç½‘æœç´¢æŠ¥é”™: {e}")

    platforms = [
        {"title": "ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹â€œä¸Šæµ·å¤§å­¦â€Bç«™æœ€æ–°è§†é¢‘", "source": "Bilibili", "url": "https://search.bilibili.com/all?keyword=ä¸Šæµ·å¤§å­¦&order=pubdate", "time": "å®æ—¶", "tag": "video"},
        {"title": "ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹â€œä¸Šæµ·å¤§å­¦â€çŸ¥ä¹å®æ—¶è®¨è®º", "source": "çŸ¥ä¹", "url": "https://www.zhihu.com/search?type=content&q=ä¸Šæµ·å¤§å­¦", "time": "å®æ—¶", "tag": "forum"},
        {"title": "ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹â€œä¸Šæµ·å¤§å­¦â€å¾®åšçƒ­æœ", "source": "å¾®åš", "url": "https://s.weibo.com/weibo?q=ä¸Šæµ·å¤§å­¦&xsort=hot", "time": "å®æ—¶", "tag": "forum"},
    ]
    return news_list[:12] + platforms

def save_to_js(data):
    path = "data.js"
    # è¿™é‡ŒåŠ ä¸€ä¸ªéšæœºæ•°ï¼Œé˜²æ­¢æ–‡ä»¶å¤§å°å®Œå…¨ä¸€æ ·å¯¼è‡´Gitä¸æäº¤
    update_info = f"{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}"
    output = {
        "update_time": update_info,
        "news": data
    }
    content = f"window.SHU_DATA = {json.dumps(output, ensure_ascii=False, indent=2)};"
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"æ•°æ®å·²æ›´æ–°: {update_info}")

if __name__ == "__main__":
    print("--- GitHub Action å¼€å§‹æ‰§è¡Œ ---")
    official = get_shu_official()
    internet = get_internet_buzz()
    save_to_js(official + internet)
    print("--- æ‰§è¡Œå®Œæ¯• ---")