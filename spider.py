import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io
import datetime
import re

# 1. åŸºç¡€é…ç½®
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
MAX_DAYS_AGO = 2  # åªä¿ç•™æœ€è¿‘ 48 å°æ—¶å†…çš„æŠ¥é“

def get_header():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.baidu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

# 2. æ—¶é—´æ¸…æ´— (åªç•™æ–°é²œçš„)
def parse_baidu_time(time_str):
    now = datetime.datetime.now()
    time_str = str(time_str).strip()
    try:
        if "åˆ†é’Ÿå‰" in time_str:
            mins = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(minutes=mins)
        elif "å°æ—¶å‰" in time_str:
            hours = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(hours=hours)
        elif "æ˜¨å¤©" in time_str:
            return now - datetime.timedelta(days=1)
        elif "å¤©å‰" in time_str:
            days = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(days=days)
        elif "å¹´" in time_str or "-" in time_str:
            clean_str = time_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            return datetime.datetime.strptime(clean_str, "%Y-%m-%d")
        else:
            return now
    except:
        return now - datetime.timedelta(days=365)

# ==========================================
# 3. å››å¤§æ ¸å¿ƒæˆ˜åŒº (è¦†ç›–æƒå¨ã€åœ°æ–¹ã€ä¸“ä¸šåª’ä½“)
# ==========================================
SEARCH_ZONES = [
    # --- æˆ˜åŒº A: æƒå¨å¤®åª’ (æœ€é«˜å…³æ³¨åº¦) ---
    {
        "name": "å¤®åª’æŠ¥é“",
        # é€»è¾‘ï¼šæœç´¢â€œä¸Šæµ·å¤§å­¦â€åŒæ—¶å¿…é¡»åŒ…å«è¿™äº›åª’ä½“åä¹‹ä¸€
        "query": 'ä¸Šæµ·å¤§å­¦ ("äººæ°‘æ—¥æŠ¥" | "å…‰æ˜æ—¥æŠ¥" | "æ–°åç½‘" | "ä¸­å›½æ—¥æŠ¥")',
        "tag": "media"
    },
    # --- æˆ˜åŒº B: ä¸»æµæ²ªåª’ (æœ¬åœ°å½±å“åŠ›) ---
    {
        "name": "æ²ªåª’èšç„¦",
        # é€»è¾‘ï¼šæ–‡æ±‡æŠ¥ã€è§£æ”¾æ—¥æŠ¥ã€æ¾æ¹ƒæ–°é—»
        "query": 'ä¸Šæµ·å¤§å­¦ ("æ–‡æ±‡æŠ¥" | "è§£æ”¾æ—¥æŠ¥" | "æ¾æ¹ƒ")',
        "tag": "media"
    },
    # --- æˆ˜åŒº C: ä¸“ä¸š/å­¦æœ¯æŠ¥åˆŠ (ä½“ç°ç§‘ç ”å®åŠ›) ---
    {
        "name": "å­¦æœ¯ä¸“ä¸š",
        # é€»è¾‘ï¼šç¤¾ç§‘æŠ¥ã€ç§‘æŠ€æŠ¥
        "query": 'ä¸Šæµ·å¤§å­¦ ("ä¸­å›½ç¤¾ä¼šç§‘å­¦æŠ¥" | "ä¸­å›½ç§‘å­¦æŠ¥" | "ç§‘æŠ€æ—¥æŠ¥")',
        "tag": "media"
    },
    # --- æˆ˜åŒº D: å®˜ç½‘ç›´é€š (æ ¡å†…åŠ¨æ€) ---
    {
        "name": "å®˜ç½‘", 
        "query": "OFFICIAL_SITE", # ç‰¹æ®Šæ ‡è®°ï¼Œèµ°ä¸“é—¨å‡½æ•°
        "tag": "official"
    }
]

def fetch_baidu_news(zone):
    """é€šç”¨ç™¾åº¦æ–°é—»æŠ“å–å™¨"""
    print(f"æ­£åœ¨æ‰«æï¼š[{zone['name']}] ...")
    news_pool = []
    
    # rtt=1 å¼ºåˆ¶æŒ‰æ—¶é—´æ’åº
    url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={zone['query']}"
    
    try:
        res = requests.get(url, headers=get_header(), timeout=12)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        items = soup.find_all('div', class_='result-op')
        if not items: items = soup.find_all('div', class_='result')
        
        for item in items:
            try:
                title_node = item.find('h3').find('a')
                title = title_node.get_text(strip=True)
                link = title_node['href']
                
                # æ¥æºæ¸…æ´—
                source_node = item.find('span', class_='c-color-gray')
                source = source_node.get_text(strip=True) if source_node else "åª’ä½“æŠ¥é“"
                
                time_node = item.find('span', class_='c-color-gray2')
                time_str = time_node.get_text(strip=True) if time_node else ""
                
                # æ—¶é—´è¿‡æ»¤
                real_time = parse_baidu_time(time_str)
                if (datetime.datetime.now() - real_time).days > MAX_DAYS_AGO:
                    continue
                
                news_pool.append({
                    "title": title, "url": link, "source": source,
                    "time": time_str, "timestamp": real_time, 
                    "tag": zone['tag']
                })
            except: continue
        time.sleep(1.5)
    except Exception as e:
        print(f"  æœç´¢é”™è¯¯: {e}")
        
    return news_pool

def get_shu_official():
    """ä¸“é—¨æŠ“å–å®˜ç½‘"""
    print("æ­£åœ¨æ‰«æï¼š[ä¸Šå¤§å®˜ç½‘] ...")
    url = "https://news.shu.edu.cn/index/zhxw.htm"
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            if title and href and len(title) > 10 and '.htm' in href:
                if "ç‰ˆæƒ" in title: continue
                if not href.startswith('http'):
                    href = f"https://news.shu.edu.cn/{href.replace('../', '')}"
                
                # å®˜ç½‘é»˜è®¤ä¸ºæœ€æ–°
                fake_time = datetime.datetime.now()
                if not any(n['url'] == href for n in news_list):
                    news_list.append({
                        "title": title, "url": href, "source": "ä¸Šå¤§å®˜ç½‘",
                        "time": "æ ¡å†…", "timestamp": fake_time, "tag": "official"
                    })
    except Exception as e:
        print(f"å®˜ç½‘é”™è¯¯: {e}")
    return news_list[:6]

def fetch_all():
    all_news = []
    
    for zone in SEARCH_ZONES:
        if zone['query'] == "OFFICIAL_SITE":
            all_news.extend(get_shu_official())
        else:
            all_news.extend(fetch_baidu_news(zone))
    
    # å›ºå®šé™æ€å…¥å£
    now = datetime.datetime.now()
    static_links = [
        {"title": "ğŸ‘‰ã€Bç«™ã€‘ä¸Šæµ·å¤§å­¦å®˜æ–¹è§†é¢‘åŠ¨æ€ (æŒ‰å‘å¸ƒæ’åº)", "source": "Bilibili", "url": "https://search.bilibili.com/all?keyword=ä¸Šæµ·å¤§å­¦&order=pubdate", "time": "å®æ—¶", "tag": "video", "timestamp": now},
        {"title": "ğŸ‘‰ã€å¾®åšã€‘ä¸Šæµ·å¤§å­¦å®æ—¶çƒ­æœå¹¿åœº", "source": "å¾®åš", "url": "https://s.weibo.com/weibo?q=ä¸Šæµ·å¤§å­¦&xsort=hot", "time": "å®æ—¶", "tag": "forum", "timestamp": now},
    ]
    
    final_list = static_links + all_news
    
    # å»é‡
    seen = set()
    unique_list = []
    for item in final_list:
        if item['title'] not in seen:
            unique_list.append(item)
            seen.add(item['title'])
            
    # æŒ‰æ—¶é—´å€’åº
    unique_list.sort(key=lambda x: x['timestamp'], reverse=True)
    
    # æ¸…ç†å­—æ®µ
    for item in unique_list:
        del item['timestamp']
        
    return unique_list[:40]

def save(data):
    try:
        # åŒ—äº¬æ—¶é—´
        utc_now = datetime.datetime.utcnow()
        cst_now = utc_now + datetime.timedelta(hours=8)
        time_str = cst_now.strftime('%Y-%m-%d %H:%M')
        
        # å˜é‡å SHU_DATA (ä¸Šå¤§å¤´æ¡ä¸“ç”¨)
        output = { "update_time": time_str, "news": data }
        
        with open("data.js", "w", encoding="utf-8") as f:
            f.write(f"window.SHU_DATA = {json.dumps(output, ensure_ascii=False, indent=2)};")
        print(f"âœ… æ›´æ–°å®Œæˆã€‚æ—¶é—´: {time_str}ï¼Œå…± {len(data)} æ¡æ–°é—»ã€‚")
    except Exception as e:
        sys.exit(1)

if __name__ == "__main__":
    data = fetch_all()
    save(data)

