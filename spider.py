import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io

# ==========================================
# æ ¸å¿ƒä¿®å¤ï¼šé˜²æ­¢äº‘ç«¯æŽ§åˆ¶å°å› ä¸ºä¸­æ–‡/EmojiæŠ¥é”™
# ==========================================
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def get_header():
    # æ¨¡æ‹ŸçœŸå®žæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«æ‹¦æˆª
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Referer': 'https://www.baidu.com/'
    }

def get_shu_official():
    """æŠ“å–å®˜ç½‘æ–°é—»"""
    url = "https://news.shu.edu.cn/index/zhxw.htm"
    print(f"Fetching Official: {url}") # çº¯è‹±æ–‡æ‰“å°ï¼Œé˜²æ­¢æŠ¥é”™
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # å°è¯•é€‚é…å¤šç§åˆ—è¡¨ç»“æž„
        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            # ç­›é€‰é€»è¾‘
            if title and href and len(title) > 8 and '.htm' in href:
                if "ç‰ˆæƒ" in title or "è”ç³»" in title: continue
                
                if not href.startswith('http'):
                    # ä¿®å¤ç›¸å¯¹è·¯å¾„
                    clean_href = href.replace('../', '').lstrip('/')
                    href = f"https://news.shu.edu.cn/{clean_href}"
                
                # ç®€å•åŽ»é‡
                if not any(n['url'] == href for n in news_list):
                    news_list.append({
                        "title": title, "url": href, 
                        "source": "ä¸Šå¤§å®˜ç½‘", "time": "æ ¡å†…", "tag": "official"
                    })
                    
        print(f"  - Official news count: {len(news_list)}")
    except Exception as e:
        print(f"  - Official Error: {e}")
    
    return news_list[:8]

def get_internet_buzz():
    """æŠ“å–å…¨ç½‘èµ„è®¯ (ç™¾åº¦æ–°é—»)"""
    url = "https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd=ä¸Šæµ·å¤§å­¦"
    print(f"Fetching Internet: {url}")
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        results = soup.find_all('div', class_='result-op')
        if not results: results = soup.find_all('div', class_='result')
        
        for item in results:
            try:
                title_tag = item.find('h3')
                if not title_tag: continue
                link_tag = title_tag.find('a')
                if not link_tag: continue
                
                title = link_tag.get_text(strip=True)
                href = link_tag['href']
                
                source_tag = item.find('span', class_='c-color-gray')
                source = source_tag.get_text(strip=True) if source_tag else "äº’è”ç½‘"
                
                time_tag = item.find('span', class_='c-color-gray2')
                pub_time = time_tag.get_text(strip=True) if time_tag else "è¿‘æœŸ"

                news_list.append({
                    "title": title, "url": href, 
                    "source": source, "time": pub_time, "tag": "media"
                })
            except:
                continue
                
        print(f"  - Internet news count: {len(news_list)}")
    except Exception as e:
        print(f"  - Internet Error: {e}")

    # é™æ€é“¾æŽ¥ (å³ä½¿çˆ¬è™«æŒ‚äº†ï¼Œè¿™äº›ä¹Ÿä¼šæ˜¾ç¤º)
    print("Adding static links...")
    platforms = [
        {"title": "ðŸ‘‰ ç‚¹å‡»æŸ¥çœ‹â€œä¸Šæµ·å¤§å­¦â€Bç«™æœ€æ–°è§†é¢‘", "source": "Bilibili", "url": "https://search.bilibili.com/all?keyword=ä¸Šæµ·å¤§å­¦&order=pubdate", "time": "å®žæ—¶", "tag": "video"},
        {"title": "ðŸ‘‰ ç‚¹å‡»æŸ¥çœ‹â€œä¸Šæµ·å¤§å­¦â€çŸ¥ä¹Žå®žæ—¶è®¨è®º", "source": "çŸ¥ä¹Ž", "url": "https://www.zhihu.com/search?type=content&q=ä¸Šæµ·å¤§å­¦", "time": "å®žæ—¶", "tag": "forum"},
        {"title": "ðŸ‘‰ ç‚¹å‡»æŸ¥çœ‹â€œä¸Šæµ·å¤§å­¦â€å¾®åšçƒ­æœ", "source": "å¾®åš", "url": "https://s.weibo.com/weibo?q=ä¸Šæµ·å¤§å­¦&xsort=hot", "time": "å®žæ—¶", "tag": "forum"},
    ]
    
    return news_list[:15] + platforms

def save_to_js(data):
    try:
        path = "data.js"
        # å†™å…¥ UTC æ—¶é—´ï¼Œå‰ç«¯ä¼šæ˜¾ç¤º
        update_info = time.strftime('%Y-%m-%d %H:%M', time.localtime())
        
        output = {
            "update_time": update_info,
            "news": data
        }
        
        # å¼ºåˆ¶ UTF-8 å†™å…¥
        content = f"window.SHU_DATA = {json.dumps(output, ensure_ascii=False, indent=2)};"
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
            
        print(f"Success! Saved {len(data)} items.")
        
    except Exception as e:
        print(f"Save Error: {e}")
        sys.exit(1) # å¦‚æžœä¿å­˜å¤±è´¥ï¼Œæ‰æŠ¥é”™çº¢è‰² X

if __name__ == "__main__":
    try:
        print(">>> Job Started")
        official = get_shu_official()
        internet = get_internet_buzz()
        
        # åˆå¹¶æ•°æ®
        all_data = official + internet
        
        # å°±ç®—æ²¡æŠ“åˆ°æ–°é—»ï¼Œè‡³å°‘æŠŠé™æ€é“¾æŽ¥å­˜è¿›åŽ»ï¼Œä¿è¯é¡µé¢ä¸ç™½æ¿
        if not all_data:
            print("Warning: No news fetched, using backup data.")
        
        save_to_js(all_data)
        print(">>> Job Finished")
        
    except Exception as e:
        print(f"Critical Error: {e}")
        sys.exit(1)
