import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io
import datetime
import re

# é˜²æ­¢äº‘ç«¯æ§åˆ¶å°æŠ¥é”™
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# --- é…ç½® ---
# åªä¿ç•™æœ€è¿‘ 2 å¤©å†…çš„æ–°é—»
MAX_DAYS_AGO = 2 

def get_header():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.baidu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

def parse_baidu_time(time_str):
    """
    æŠŠå„ç§æ ¼å¼çš„æ—¶é—´ï¼ˆ5åˆ†é’Ÿå‰ã€æ˜¨å¤©ã€2025-12-15ï¼‰ç»Ÿä¸€è½¬æ¢æˆ datetime å¯¹è±¡
    ä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒå’Œæ’åº
    """
    now = datetime.datetime.now()
    time_str = time_str.strip()

    try:
        if "åˆ†é’Ÿå‰" in time_str:
            mins = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(minutes=mins)
        elif "å°æ—¶å‰" in time_str:
            hours = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(hours=hours)
        elif "æ˜¨å¤©" in time_str:
            return now - datetime.timedelta(days=1)
        elif "å‰å¤©" in time_str:
            return now - datetime.timedelta(days=2)
        elif "å¤©å‰" in time_str:
            days = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(days=days)
        elif "å¹´" in time_str or "-" in time_str:
            # å¤„ç† 2025å¹´12æœˆ15æ—¥ æˆ– 2025-12-15
            clean_str = time_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            return datetime.datetime.strptime(clean_str, "%Y-%m-%d")
        else:
            # æ— æ³•è¯†åˆ«çš„æ ¼å¼ï¼ˆæ¯”å¦‚â€œåˆšåˆšâ€ï¼‰ï¼Œé»˜è®¤ç®—ä½œç°åœ¨
            return now
    except:
        return now - datetime.timedelta(days=365) # å‡ºé”™å°±å½“åšæ—§æ–°é—»å¤„ç†

def get_shu_official():
    """ä¸Šå¤§å®˜ç½‘æ–°é—» (å®˜ç½‘é€šå¸¸æŒ‰æ—¶é—´æ’ï¼Œç›´æ¥å–å‰8æ¡)"""
    url = "https://news.shu.edu.cn/index/zhxw.htm"
    print(f"æ‰«æå®˜ç½‘: {url}")
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        links = soup.find_all('a')
        
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if title and href and len(title) > 10 and '.htm' in href:
                if "ç‰ˆæƒ" in title: continue
                
                if not href.startswith('http'):
                    href = f"https://news.shu.edu.cn/{href.replace('../', '')}"
                
                # å®˜ç½‘æ²¡å†™å…·ä½“æ—¶é—´ï¼Œé»˜è®¤ç®—ä½œæœ€æ–°ï¼Œæ’åœ¨æœ€å‰
                # ä¸ºäº†æ’åºï¼Œç»™ä»–ä¸€ä¸ªç¨å¾®æ»åä¸€ç‚¹ç‚¹çš„â€œå½“å‰æ—¶é—´â€
                fake_time = datetime.datetime.now()
                
                if not any(n['url'] == href for n in news_list):
                    news_list.append({
                        "title": title, "url": href, "source": "ä¸Šå¤§å®˜ç½‘", 
                        "time_str": "æ ¡å†…æœ€æ–°", # æ˜¾ç¤ºç»™ç”¨æˆ·çœ‹çš„
                        "timestamp": fake_time, # æ’åºç”¨çš„
                        "tag": "official"
                    })
    except Exception as e:
        print(f"å®˜ç½‘æŠ“å–é”™è¯¯: {e}")
    return news_list[:8]

def get_internet_buzz():
    """å…¨ç½‘æœç´¢ (å¼ºåˆ¶æŒ‰æ—¶é—´æ’åº)"""
    # å…³é”®å‚æ•° rtt=1 (Sort by Time)ï¼Œé»˜è®¤æ˜¯4 (Sort by Relevance)
    url = "https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd=ä¸Šæµ·å¤§å­¦"
    print(f"å…¨ç½‘æ£€ç´¢ (å·²å¼€å¯æ—¶é—´å¼ºæ’åº): {url}")
    
    news_list = []
    try:
        res = requests.get(url, headers=get_header(), timeout=12)
        soup = BeautifulSoup(res.text, 'html.parser')
        results = soup.find_all('div', class_='result-op')
        
        for item in results:
            try:
                title_node = item.find('h3').find('a')
                title = title_node.get_text(strip=True)
                href = title_node['href']
                
                source_node = item.find('span', class_='c-color-gray')
                source = source_node.get_text(strip=True) if source_node else "äº’è”ç½‘"
                
                time_node = item.find('span', class_='c-color-gray2')
                time_str = time_node.get_text(strip=True) if time_node else ""

                # --- å…³é”®æ­¥éª¤ï¼šæ—¶é—´è¿‡æ»¤ ---
                real_time = parse_baidu_time(time_str)
                days_diff = (datetime.datetime.now() - real_time).days
                
                # å¦‚æœæ–°é—»è¶…è¿‡äº† 2 å¤©ï¼Œç›´æ¥æ‰”æ‰ (continue)
                if days_diff > MAX_DAYS_AGO:
                    continue

                news_list.append({
                    "title": title, "url": href, "source": source, 
                    "time_str": time_str, # åŸæ ·æ˜¾ç¤º "5åˆ†é’Ÿå‰"
                    "timestamp": real_time, # æ’åºç”¨
                    "tag": "media"
                })
            except:
                continue
    except Exception as e:
        print(f"å…¨ç½‘æŠ“å–é”™è¯¯: {e}")
        
    return news_list

def save_to_js(data):
    # --- æœ€ç»ˆæ’åºï¼šæŒ‰æ—¶é—´æˆ³å€’åº (æœ€æ–°çš„åœ¨æœ€ä¸Šé¢) ---
    data.sort(key=lambda x: x['timestamp'], reverse=True)
    
    # æ¸…ç†æ‰ timestamp å­—æ®µï¼ˆä¸éœ€è¦å†™å…¥JSæ–‡ä»¶ï¼‰
    for item in data:
        del item['timestamp']
        
    # åŒ—äº¬æ—¶é—´
    utc_now = datetime.datetime.utcnow()
    cst_now = utc_now + datetime.timedelta(hours=8)
    time_str = cst_now.strftime('%Y-%m-%d %H:%M')

    output = {
        "update_time": time_str,
        "news": data[:20] # åªä¿ç•™æœ€æ–°çš„20æ¡
    }
    
    with open("data.js", "w", encoding="utf-8") as f:
        f.write(f"window.SHU_DATA = {json.dumps(output, ensure_ascii=False, indent=2)};")
    print(f"âœ… æ›´æ–°å®Œæˆã€‚æ—¶é—´: {time_str}ï¼Œå…± {len(data)} æ¡æ–°é—»ã€‚")

if __name__ == "__main__":
    official = get_shu_official()
    internet = get_internet_buzz()
    
    # é™æ€é“¾æ¥ (æ”¾åœ¨æœ€å)
    static_links = [
        {"title": "ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹ Bç«™â€œä¸Šæµ·å¤§å­¦â€æœ€æ–°è§†é¢‘ (æŒ‰å‘å¸ƒæ—¶é—´)", "source": "Bilibili", "url": "https://search.bilibili.com/all?keyword=ä¸Šæµ·å¤§å­¦&order=pubdate", "time_str": "å®æ—¶", "tag": "video", "timestamp": datetime.datetime.now()},
        {"title": "ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹ å¾®åšâ€œä¸Šæµ·å¤§å­¦â€å®æ—¶å¹¿åœº", "source": "å¾®åš", "url": "https://s.weibo.com/weibo?q=ä¸Šæµ·å¤§å­¦&xsort=hot", "time_str": "å®æ—¶", "tag": "forum", "timestamp": datetime.datetime.now()},
    ]
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = official + internet + static_links
    save_to_js(all_data)
